# -*- coding: utf-8 -*-
"""Copy of NLP sample.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eVp9XgyYmwuBvPn-gJd9PF_H6ijdvoEW
"""

# Importing all the required modules

import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords

import urllib.response
#from urllib.request import Request,urlopen
from bs4 import BeautifulSoup



website = 'https://en.wikipedia.org/wiki/SpaceX'

#website = 'https://en.wikipedia.org/wiki/Uri:_The_Surgical_Strike'

# Grab a webpage and analyze the text to see what the page is about.
# urllib module will help us to crawl the webpage

response =  urllib.request.urlopen(website)

html = response.read()

# Display content

#html

# Use Beautiful Soup which is a Python library for pulling data out of HTML and XML files. 
# Use beautiful soup to clean our webpage text of HTML tags.

soup = BeautifulSoup(html,'html5lib')
text = soup.get_text(strip = True)
#soup.find_all('p')
for paragraph in soup.find_all('p'):
    #print(paragraph.string)
    print(str(paragraph.text))
    
#text = str(paragraph.text)

# Display cleaned text

#text

# Convert the text into tokens.

tokens = [t for t in text.split()]

# Display tokens

print(tokens)



"""# POS Tagging

The primary target of Part-of-Speech(POS) tagging is to identify the grammatical group of a given word. Whether it is a NOUN, PRONOUN, ADJECTIVE, VERB, ADVERBS, etc. based on the context. POS Tagging looks for relationships within the sentence and assigns a corresponding tag to the word.
"""

nltk.download('averaged_perceptron_tagger')
nltk.pos_tag(tokens)

# Count word frequency

sr= stopwords.words('english')
clean_tokens = tokens[:]
for token in tokens:
    if token in stopwords.words('english'):
        
        clean_tokens.remove(token)                                               # Removes stopwords such as - a,an,be to avoid hindering the word count
freq = nltk.FreqDist(clean_tokens)
for key,val in freq.items():
    print(str(key) + ':' + str(val))

# Display a plot of the word frequency

freq.plot(5, cumulative=False)

























from urllib.request import urlopen
from bs4 import BeautifulSoup
#articleURL = "https://timesofindia.indiatimes.com/blogs/toi-edit-page/republic-days-gift-let-us-celebrate-constitutional-correctness-not-sectarian-morality/"
articleURL = "http://curia.europa.eu/juris/document/document.jsf?text=&docid=139407&pageIndex=0&doclang=EN&mode=lst&dir=&occ=first&part=1&cid=52454"
def getText(url):
    page = urlopen(url).read().decode('utf8', 'ignore')
    soup = BeautifulSoup(page, 'lxml')
    text = ' '.join(map(lambda p: p.text, soup.find_all('p')))
    return text.encode('ascii', errors='replace').decode().replace("?","")
text = getText(articleURL)

import nltk
nltk.download('punkt')
# nltk.download()
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords
from nltk.probability import FreqDist
from collections import defaultdict
from string import punctuation
from heapq import nlargest
def summarize(text, n):
    sents = sent_tokenize(text)
    
    assert n <= len(sents)
    wordSent = word_tokenize(text.lower())
    stopWords = set(stopwords.words('english')+list(punctuation))
    
    wordSent= [word for word in wordSent if word not in stopWords]
    freq = FreqDist(wordSent)
    ranking = defaultdict(int)
    
    for i, sent in enumerate(sents):
        for w in word_tokenize(sent.lower()):
            if w in freq:
                ranking[i] += freq[w]
    sentsIDX = nlargest(n, ranking, key=ranking.get)
    return [sents[j] for j in sorted(sentsIDX)]
summaryArr = summarize(text, 10)
# summaryArr

summaryArr

freq.plot(20, cumulative=False)

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
import numpy as np
vectorizer = TfidfVectorizer(max_df=0.5,min_df=2,stop_words='english')
X = vectorizer.fit_transform(summaryArr)
km = KMeans(n_clusters = 3, init = 'k-means++', max_iter = 100, n_init = 1, verbose = True)
km.fit(X)
np.unique(km.labels_, return_counts=True)
text={}
for i,cluster in enumerate(km.labels_):
    oneDocument = summaryArr[i]
    if cluster not in text.keys():
        text[cluster] = oneDocument
    else:
        text[cluster] += oneDocument
stopWords = set(stopwords.words('english')+list(punctuation))
keywords = {}
counts={}
for cluster in range(3):
    word_sent = word_tokenize(text[cluster].lower())
    word_sent=[word for word in word_sent if word not in stopWords]
    freq = FreqDist(word_sent)
    keywords[cluster] = nlargest(100, freq, key=freq.get)
    counts[cluster]=freq
uniqueKeys={}
for cluster in range(3):   
    other_clusters=list(set(range(3))-set([cluster]))
    keys_other_clusters=set(keywords[other_clusters[0]]).union(set(keywords[other_clusters[1]]))
    unique=set(keywords[cluster])-keys_other_clusters
    uniqueKeys[cluster]=nlargest(10, unique, key=counts[cluster].get)
print(uniqueKeys)

freq.plot(20, cumulative=False)































"""# Speech Recognition"""

!pip install SpeechRecognition
!python -m pip install --upgrade pip setuptools wheel
!pip install --upgrade pocketsphinx

import speech_recognition as sr

r = sr.Recognizer()

with sr.Microphone() as source:
  print("Say Something!")
  audio = r.listen(source)
  print("Time Over, Thanks")
  
try:
  print("Text: "+r.recognize_google(audio))
except:
  pass









